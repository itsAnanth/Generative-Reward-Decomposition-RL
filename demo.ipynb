{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "91c2099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from modules import GenerativeModel, ReplayBuffer, DiscreteSACAgent, EpisodicRewardWrapper\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3e53220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LR_POLICY = 3e-4          # Learning rate for actor and critic\n",
    "LR_GENERATIVE = 3e-4      # Learning rate for generative model\n",
    "GAMMA = 0.99              # Discount factor\n",
    "REPLAY_BUFFER_SIZE = 50000 # Size of the replay buffer\n",
    "BATCH_SIZE = 256          # Batch size for training\n",
    "TAU = 0.005               # Soft update coefficient for target networks\n",
    "ALPHA = 0.2               # SAC temperature parameter (entropy regularization)\n",
    "HIDDEN_DIM = 256          # Hidden dimension for neural networks\n",
    "MAX_EPISODES = 500      # Total number of episodes to run\n",
    "MAX_STEPS_PER_EPISODE = 500 # Max steps per episode for CartPole-v1\n",
    "START_TRAINING_EPISODES = 10 # Number of episodes to collect data before training starts\n",
    "\n",
    "# Hyperparameters for the GRD generative model loss (L_reg)\n",
    "# These control the sparsity of the learned causal graph. Increased to encourage sparsity.\n",
    "LAMBDA_S_R = 5e-4  # state -> reward\n",
    "LAMBDA_A_R = 1e-5  # action -> reward\n",
    "LAMBDA_S_S = 5e-5  # state -> state\n",
    "LAMBDA_A_S = 1e-8  # action -> state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bfe392d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import EpisodicRewardWrapper\n",
    "env = gym.make(\"CartPole-v1\", render_mode='human')\n",
    "env = EpisodicRewardWrapper(env)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "compact_state_dim = state_dim \n",
    "\n",
    "replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "generative_model = GenerativeModel(state_dim, action_dim, HIDDEN_DIM, DEVICE, LR_GENERATIVE, GAMMA, LAMBDA_S_S, LAMBDA_S_R, LAMBDA_A_S, LAMBDA_A_R)\n",
    "sac_agent = DiscreteSACAgent(state_dim, action_dim, compact_state_dim, HIDDEN_DIM, DEVICE, LR_POLICY, TAU, GAMMA, ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "18785159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_equal(model1, model2, rtol=1e-05, atol=1e-08):\n",
    "    for p1, p2 in zip(model1.parameters(), model2.parameters()):\n",
    "        if not torch.allclose(p1, p2, rtol=rtol, atol=atol):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "99620f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model.load('weights', DEVICE)\n",
    "sac_agent.load('weights', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bceedb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Causal Probs (S->R): ['1.00', '1.00', '1.00', '1.00']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    C_s_s, _, C_s_r, C_a_r = generative_model.causal_module.get_causal_masks(training=False)\n",
    "    compact_mask = generative_model.causal_module.get_compact_representation_mask(C_s_s, C_s_r)\n",
    "    s_r_probs = F.softmax(generative_model.causal_module.s_to_r_logits, dim=-1)[:, 1].cpu().numpy()\n",
    "    a_r_probs = F.softmax(generative_model.causal_module.a_to_r_logits, dim=-1)[:, 1].cpu().numpy()\n",
    "    print(f\"  Causal Probs (S->R): {[f'{p:.2f}' for p in s_r_probs]}\")\n",
    "    print(f\"  Compact Mask: {compact_mask.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9223ae6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "state, _ = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "\n",
    "episode_reward = 0\n",
    "\n",
    "for t in range(MAX_STEPS_PER_EPISODE):\n",
    "    with torch.no_grad():\n",
    "        C_s_s, _, C_s_r, _ = generative_model.causal_module.get_causal_masks(training=False)\n",
    "        compact_mask = generative_model.causal_module.get_compact_representation_mask(C_s_s, C_s_r)\n",
    "        compact_state = state * compact_mask\n",
    "        \n",
    "    action = sac_agent.select_action(compact_state)\n",
    "    next_state_np, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "\n",
    "    episode_reward += reward \n",
    "    state = torch.tensor(next_state_np, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "    \n",
    "    if done:\n",
    "        print(\"done\")\n",
    "        break\n",
    "    \n",
    "    \n",
    "\n",
    "print(f\"Reward: {episode_reward}\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
