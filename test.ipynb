{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc02c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu guy run this code and make sure weights are saved in /weights folder, then push to repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f952861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "State dim: 4, Action dim: 2\n",
      "Epi 1/200 | Avg Steps (last 100): 16.00 | Steps: 16 | Time: 0.04s\n",
      "Reward: 16.0\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 0. 0. 0.]\n",
      "Epi 2/200 | Avg Steps (last 100): 14.50 | Steps: 13 | Time: 0.07s\n",
      "Reward: 13.0\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 3/200 | Avg Steps (last 100): 13.33 | Steps: 11 | Time: 0.11s\n",
      "Reward: 11.0\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 0.]\n",
      "Epi 4/200 | Avg Steps (last 100): 18.50 | Steps: 34 | Time: 0.24s\n",
      "Reward: 34.0\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [0. 1. 0. 1.]\n",
      "Epi 5/200 | Avg Steps (last 100): 17.60 | Steps: 14 | Time: 0.27s\n",
      "Reward: 14.0\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 1. 0. 0.]\n",
      "Epi 6/200 | Avg Steps (last 100): 17.17 | Steps: 15 | Time: 0.30s\n",
      "Reward: 15.0\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 7/200 | Avg Steps (last 100): 21.14 | Steps: 45 | Time: 0.39s\n",
      "Reward: 45.0\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 1. 0. 1.]\n",
      "Epi 8/200 | Avg Steps (last 100): 20.50 | Steps: 16 | Time: 0.42s\n",
      "Reward: 16.0\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 9/200 | Avg Steps (last 100): 19.33 | Steps: 10 | Time: 0.44s\n",
      "Reward: 10.0\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 10/200 | Avg Steps (last 100): 19.30 | Steps: 19 | Time: 0.48s\n",
      "Reward: 19.0\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 11/200 | Avg Steps (last 100): 18.55 | Steps: 11 | Time: 0.50s\n",
      "Reward: 11.0\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 12/200 | Avg Steps (last 100): 18.25 | Steps: 15 | Time: 0.53s\n",
      "Reward: 15.0\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\duk\\RL\\Generative-Reward-Decomposition-RL\\modules\\GenerativeModel.py:51: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  rew_losses.append(F.mse_loss(predicted_return, total_return))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epi 13/200 | Avg Steps (last 100): 20.69 | Steps: 50 | Time: 1.06s\n",
      "Reward: 50.0\n",
      "  Losses -> Gen: 101.6207 (Rew: 101.5593, Dyn: 0.0598, Reg: 0.0017)\n",
      "  Policy -> Actor: -0.5626, Critic: 0.2991\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 14/200 | Avg Steps (last 100): 20.36 | Steps: 16 | Time: 1.64s\n",
      "Reward: 16.0\n",
      "  Losses -> Gen: 11.7855 (Rew: 11.7477, Dyn: 0.0362, Reg: 0.0017)\n",
      "  Policy -> Actor: -1.2190, Critic: 0.4764\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 15/200 | Avg Steps (last 100): 21.00 | Steps: 30 | Time: 2.67s\n",
      "Reward: 30.0\n",
      "  Losses -> Gen: 4.9944 (Rew: 4.9206, Dyn: 0.0721, Reg: 0.0017)\n",
      "  Policy -> Actor: -1.5068, Critic: 0.0750\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 16/200 | Avg Steps (last 100): 20.31 | Steps: 10 | Time: 3.03s\n",
      "Reward: 10.0\n",
      "  Losses -> Gen: 4.4315 (Rew: 4.4189, Dyn: 0.0109, Reg: 0.0017)\n",
      "  Policy -> Actor: -1.7071, Critic: 0.1223\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [0. 0. 0. 0.]\n",
      "Epi 17/200 | Avg Steps (last 100): 20.41 | Steps: 22 | Time: 3.99s\n",
      "Reward: 22.0\n",
      "  Losses -> Gen: 0.1549 (Rew: 0.1443, Dyn: 0.0089, Reg: 0.0017)\n",
      "  Policy -> Actor: -1.7874, Critic: 0.0396\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 18/200 | Avg Steps (last 100): 20.89 | Steps: 29 | Time: 5.13s\n",
      "Reward: 29.0\n",
      "  Losses -> Gen: 2.6201 (Rew: 2.5975, Dyn: 0.0209, Reg: 0.0017)\n",
      "  Policy -> Actor: -1.9235, Critic: 0.0552\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 19/200 | Avg Steps (last 100): 20.32 | Steps: 10 | Time: 5.51s\n",
      "Reward: 10.0\n",
      "  Losses -> Gen: 3.4694 (Rew: 3.4601, Dyn: 0.0076, Reg: 0.0017)\n",
      "  Policy -> Actor: -2.0251, Critic: 0.0818\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 20/200 | Avg Steps (last 100): 20.80 | Steps: 30 | Time: 6.92s\n",
      "Reward: 30.0\n",
      "  Losses -> Gen: 0.6295 (Rew: 0.6197, Dyn: 0.0081, Reg: 0.0017)\n",
      "  Policy -> Actor: -2.0757, Critic: 0.0607\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [0. 1. 0. 1.]\n",
      "Epi 21/200 | Avg Steps (last 100): 20.62 | Steps: 17 | Time: 7.65s\n",
      "Reward: 17.0\n",
      "  Losses -> Gen: 0.3805 (Rew: 0.3729, Dyn: 0.0059, Reg: 0.0017)\n",
      "  Policy -> Actor: -2.1423, Critic: 0.0829\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 0.]\n",
      "Epi 22/200 | Avg Steps (last 100): 20.18 | Steps: 11 | Time: 8.07s\n",
      "Reward: 11.0\n",
      "  Losses -> Gen: 0.3666 (Rew: 0.3563, Dyn: 0.0086, Reg: 0.0017)\n",
      "  Policy -> Actor: -2.2858, Critic: 0.1219\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.34', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 23/200 | Avg Steps (last 100): 20.26 | Steps: 22 | Time: 8.98s\n",
      "Reward: 22.0\n",
      "  Losses -> Gen: 5.2226 (Rew: 5.2012, Dyn: 0.0198, Reg: 0.0017)\n",
      "  Policy -> Actor: -2.3827, Critic: 0.1528\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.35', '0.15']\n",
      "  Compact Mask: [0. 1. 0. 0.]\n",
      "Epi 24/200 | Avg Steps (last 100): 19.83 | Steps: 10 | Time: 9.36s\n",
      "Reward: 10.0\n",
      "  Losses -> Gen: 3.2594 (Rew: 3.2453, Dyn: 0.0124, Reg: 0.0017)\n",
      "  Policy -> Actor: -2.4435, Critic: 0.0917\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.35', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 25/200 | Avg Steps (last 100): 20.32 | Steps: 32 | Time: 10.64s\n",
      "Reward: 32.0\n",
      "  Losses -> Gen: 4.7231 (Rew: 4.7121, Dyn: 0.0093, Reg: 0.0017)\n",
      "  Policy -> Actor: -2.5942, Critic: 0.1571\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.35', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 26/200 | Avg Steps (last 100): 20.04 | Steps: 13 | Time: 11.12s\n",
      "Reward: 13.0\n",
      "  Losses -> Gen: 4.1117 (Rew: 4.0925, Dyn: 0.0176, Reg: 0.0017)\n",
      "  Policy -> Actor: -2.5992, Critic: 0.1726\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.35', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 27/200 | Avg Steps (last 100): 19.81 | Steps: 14 | Time: 11.64s\n",
      "Reward: 14.0\n",
      "  Losses -> Gen: 3.8988 (Rew: 3.8922, Dyn: 0.0049, Reg: 0.0017)\n",
      "  Policy -> Actor: -2.6772, Critic: 0.1474\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.35', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 28/200 | Avg Steps (last 100): 19.61 | Steps: 14 | Time: 12.16s\n",
      "Reward: 14.0\n",
      "  Losses -> Gen: 6.7048 (Rew: 6.6817, Dyn: 0.0215, Reg: 0.0017)\n",
      "  Policy -> Actor: -2.7404, Critic: 0.2682\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.35', '0.15']\n",
      "  Compact Mask: [0. 0. 0. 0.]\n",
      "Epi 29/200 | Avg Steps (last 100): 19.31 | Steps: 11 | Time: 12.57s\n",
      "Reward: 11.0\n",
      "  Losses -> Gen: 4.5773 (Rew: 4.5301, Dyn: 0.0455, Reg: 0.0017)\n",
      "  Policy -> Actor: -2.7693, Critic: 0.1833\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.35', '0.15']\n",
      "  Compact Mask: [0. 1. 0. 0.]\n",
      "Epi 30/200 | Avg Steps (last 100): 19.53 | Steps: 26 | Time: 13.54s\n",
      "Reward: 26.0\n",
      "  Losses -> Gen: 0.9728 (Rew: 0.9416, Dyn: 0.0296, Reg: 0.0017)\n",
      "  Policy -> Actor: -2.9722, Critic: 0.2155\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.35', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 0.]\n",
      "Epi 31/200 | Avg Steps (last 100): 19.77 | Steps: 27 | Time: 14.56s\n",
      "Reward: 27.0\n",
      "  Losses -> Gen: 0.5853 (Rew: 0.5597, Dyn: 0.0239, Reg: 0.0017)\n",
      "  Policy -> Actor: -3.0366, Critic: 0.2237\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.35', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 32/200 | Avg Steps (last 100): 19.69 | Steps: 17 | Time: 15.22s\n",
      "Reward: 17.0\n",
      "  Losses -> Gen: 1.9711 (Rew: 1.9629, Dyn: 0.0066, Reg: 0.0017)\n",
      "  Policy -> Actor: -3.1450, Critic: 0.2491\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.35', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 33/200 | Avg Steps (last 100): 19.79 | Steps: 23 | Time: 16.22s\n",
      "Reward: 23.0\n",
      "  Losses -> Gen: 2.6425 (Rew: 2.6357, Dyn: 0.0052, Reg: 0.0017)\n",
      "  Policy -> Actor: -3.3551, Critic: 0.2468\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.36', '0.15']\n",
      "  Compact Mask: [1. 0. 0. 0.]\n",
      "Epi 34/200 | Avg Steps (last 100): 20.47 | Steps: 43 | Time: 17.85s\n",
      "Reward: 43.0\n",
      "  Losses -> Gen: 4.1573 (Rew: 4.1307, Dyn: 0.0249, Reg: 0.0017)\n",
      "  Policy -> Actor: -3.4982, Critic: 0.2908\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.36', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 35/200 | Avg Steps (last 100): 20.83 | Steps: 33 | Time: 19.10s\n",
      "Reward: 33.0\n",
      "  Losses -> Gen: 1.1847 (Rew: 1.1704, Dyn: 0.0126, Reg: 0.0017)\n",
      "  Policy -> Actor: -3.5450, Critic: 0.3119\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.36', '0.15']\n",
      "  Compact Mask: [0. 0. 0. 0.]\n",
      "Epi 36/200 | Avg Steps (last 100): 20.89 | Steps: 23 | Time: 19.99s\n",
      "Reward: 23.0\n",
      "  Losses -> Gen: 0.8679 (Rew: 0.8629, Dyn: 0.0033, Reg: 0.0017)\n",
      "  Policy -> Actor: -3.8062, Critic: 0.3808\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.36', '0.15']\n",
      "  Compact Mask: [1. 0. 1. 1.]\n",
      "Epi 37/200 | Avg Steps (last 100): 20.70 | Steps: 14 | Time: 20.51s\n",
      "Reward: 14.0\n",
      "  Losses -> Gen: 1.7828 (Rew: 1.7725, Dyn: 0.0086, Reg: 0.0017)\n",
      "  Policy -> Actor: -3.8252, Critic: 0.5442\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.37', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 38/200 | Avg Steps (last 100): 20.53 | Steps: 14 | Time: 21.06s\n",
      "Reward: 14.0\n",
      "  Losses -> Gen: 0.1241 (Rew: 0.1165, Dyn: 0.0059, Reg: 0.0017)\n",
      "  Policy -> Actor: -3.8061, Critic: 0.3359\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.37', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 39/200 | Avg Steps (last 100): 20.62 | Steps: 24 | Time: 21.97s\n",
      "Reward: 24.0\n",
      "  Losses -> Gen: 2.5885 (Rew: 2.5817, Dyn: 0.0051, Reg: 0.0017)\n",
      "  Policy -> Actor: -3.9980, Critic: 0.5236\n",
      "  Causal Probs (S->R): ['0.29', '0.73', '0.37', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 40/200 | Avg Steps (last 100): 20.88 | Steps: 31 | Time: 23.14s\n",
      "Reward: 31.0\n",
      "  Losses -> Gen: 1.6359 (Rew: 1.6282, Dyn: 0.0060, Reg: 0.0017)\n",
      "  Policy -> Actor: -4.1271, Critic: 0.5600\n",
      "  Causal Probs (S->R): ['0.29', '0.74', '0.37', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 41/200 | Avg Steps (last 100): 21.24 | Steps: 36 | Time: 24.52s\n",
      "Reward: 36.0\n",
      "  Losses -> Gen: 0.8927 (Rew: 0.8782, Dyn: 0.0128, Reg: 0.0017)\n",
      "  Policy -> Actor: -4.3723, Critic: 0.7393\n",
      "  Causal Probs (S->R): ['0.29', '0.74', '0.37', '0.15']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 42/200 | Avg Steps (last 100): 21.50 | Steps: 32 | Time: 25.74s\n",
      "Reward: 32.0\n",
      "  Losses -> Gen: 0.5581 (Rew: 0.5471, Dyn: 0.0093, Reg: 0.0017)\n",
      "  Policy -> Actor: -4.4093, Critic: 0.6958\n",
      "  Causal Probs (S->R): ['0.29', '0.74', '0.37', '0.15']\n",
      "  Compact Mask: [1. 1. 0. 0.]\n",
      "Epi 43/200 | Avg Steps (last 100): 22.95 | Steps: 84 | Time: 28.88s\n",
      "Reward: 84.0\n",
      "  Losses -> Gen: 0.9230 (Rew: 0.9119, Dyn: 0.0094, Reg: 0.0017)\n",
      "  Policy -> Actor: -5.0812, Critic: 1.3325\n",
      "  Causal Probs (S->R): ['0.29', '0.74', '0.38', '0.15']\n",
      "  Compact Mask: [1. 1. 0. 0.]\n",
      "Epi 44/200 | Avg Steps (last 100): 23.39 | Steps: 42 | Time: 30.44s\n",
      "Reward: 42.0\n",
      "  Losses -> Gen: 12.8660 (Rew: 12.8549, Dyn: 0.0093, Reg: 0.0017)\n",
      "  Policy -> Actor: -5.3721, Critic: 0.6523\n",
      "  Causal Probs (S->R): ['0.29', '0.74', '0.38', '0.15']\n",
      "  Compact Mask: [1. 1. 0. 0.]\n",
      "Epi 45/200 | Avg Steps (last 100): 23.51 | Steps: 29 | Time: 31.52s\n",
      "Reward: 29.0\n",
      "  Losses -> Gen: 1.2744 (Rew: 1.2689, Dyn: 0.0038, Reg: 0.0017)\n",
      "  Policy -> Actor: -5.2724, Critic: 1.1151\n",
      "  Causal Probs (S->R): ['0.29', '0.74', '0.38', '0.16']\n",
      "  Compact Mask: [0. 0. 0. 0.]\n",
      "Epi 46/200 | Avg Steps (last 100): 23.43 | Steps: 20 | Time: 32.26s\n",
      "Reward: 20.0\n",
      "  Losses -> Gen: 4.3441 (Rew: 4.3293, Dyn: 0.0130, Reg: 0.0017)\n",
      "  Policy -> Actor: -5.3214, Critic: 1.1188\n",
      "  Causal Probs (S->R): ['0.29', '0.74', '0.38', '0.16']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 47/200 | Avg Steps (last 100): 23.43 | Steps: 23 | Time: 33.14s\n",
      "Reward: 23.0\n",
      "  Losses -> Gen: 0.8646 (Rew: 0.8120, Dyn: 0.0508, Reg: 0.0017)\n",
      "  Policy -> Actor: -5.4270, Critic: 0.5056\n",
      "  Causal Probs (S->R): ['0.29', '0.74', '0.39', '0.16']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 48/200 | Avg Steps (last 100): 23.73 | Steps: 38 | Time: 34.64s\n",
      "Reward: 38.0\n",
      "  Losses -> Gen: 6.3584 (Rew: 6.3489, Dyn: 0.0078, Reg: 0.0017)\n",
      "  Policy -> Actor: -5.9527, Critic: 1.6809\n",
      "  Causal Probs (S->R): ['0.29', '0.74', '0.39', '0.16']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 49/200 | Avg Steps (last 100): 23.88 | Steps: 31 | Time: 35.82s\n",
      "Reward: 31.0\n",
      "  Losses -> Gen: 1.6657 (Rew: 1.6529, Dyn: 0.0110, Reg: 0.0017)\n",
      "  Policy -> Actor: -5.6387, Critic: 0.9390\n",
      "  Causal Probs (S->R): ['0.29', '0.74', '0.39', '0.16']\n",
      "  Compact Mask: [1. 1. 0. 0.]\n",
      "Epi 50/200 | Avg Steps (last 100): 24.18 | Steps: 39 | Time: 37.28s\n",
      "Reward: 39.0\n",
      "  Losses -> Gen: 1.0329 (Rew: 1.0112, Dyn: 0.0200, Reg: 0.0017)\n",
      "  Policy -> Actor: -6.1647, Critic: 1.0102\n",
      "  Causal Probs (S->R): ['0.29', '0.74', '0.39', '0.16']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 51/200 | Avg Steps (last 100): 24.20 | Steps: 25 | Time: 38.22s\n",
      "Reward: 25.0\n",
      "  Losses -> Gen: 3.0223 (Rew: 3.0131, Dyn: 0.0074, Reg: 0.0017)\n",
      "  Policy -> Actor: -6.3679, Critic: 0.8195\n",
      "  Causal Probs (S->R): ['0.29', '0.74', '0.39', '0.16']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 52/200 | Avg Steps (last 100): 24.50 | Steps: 40 | Time: 39.72s\n",
      "Reward: 40.0\n",
      "  Losses -> Gen: 0.3763 (Rew: 0.3398, Dyn: 0.0347, Reg: 0.0017)\n",
      "  Policy -> Actor: -6.2473, Critic: 0.5733\n",
      "  Causal Probs (S->R): ['0.29', '0.74', '0.40', '0.16']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 53/200 | Avg Steps (last 100): 24.96 | Steps: 49 | Time: 41.65s\n",
      "Reward: 49.0\n",
      "  Losses -> Gen: 1.8481 (Rew: 1.8410, Dyn: 0.0054, Reg: 0.0018)\n",
      "  Policy -> Actor: -6.7065, Critic: 1.2862\n",
      "  Causal Probs (S->R): ['0.30', '0.74', '0.40', '0.16']\n",
      "  Compact Mask: [0. 0. 0. 0.]\n",
      "Epi 54/200 | Avg Steps (last 100): 24.98 | Steps: 26 | Time: 42.64s\n",
      "Reward: 26.0\n",
      "  Losses -> Gen: 1.3008 (Rew: 1.2953, Dyn: 0.0037, Reg: 0.0018)\n",
      "  Policy -> Actor: -6.5727, Critic: 1.0440\n",
      "  Causal Probs (S->R): ['0.30', '0.75', '0.40', '0.16']\n",
      "  Compact Mask: [0. 1. 0. 0.]\n",
      "Epi 55/200 | Avg Steps (last 100): 25.29 | Steps: 42 | Time: 44.22s\n",
      "Reward: 42.0\n",
      "  Losses -> Gen: 2.0452 (Rew: 2.0296, Dyn: 0.0138, Reg: 0.0018)\n",
      "  Policy -> Actor: -6.9789, Critic: 1.7360\n",
      "  Causal Probs (S->R): ['0.30', '0.75', '0.41', '0.16']\n",
      "  Compact Mask: [1. 1. 1. 0.]\n",
      "Epi 56/200 | Avg Steps (last 100): 25.59 | Steps: 42 | Time: 45.82s\n",
      "Reward: 42.0\n",
      "  Losses -> Gen: 1.2323 (Rew: 1.2265, Dyn: 0.0041, Reg: 0.0018)\n",
      "  Policy -> Actor: -7.0831, Critic: 1.1063\n",
      "  Causal Probs (S->R): ['0.30', '0.75', '0.41', '0.16']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 57/200 | Avg Steps (last 100): 25.91 | Steps: 44 | Time: 47.72s\n",
      "Reward: 44.0\n",
      "  Losses -> Gen: 2.3114 (Rew: 2.2850, Dyn: 0.0246, Reg: 0.0018)\n",
      "  Policy -> Actor: -7.3504, Critic: 1.9640\n",
      "  Causal Probs (S->R): ['0.30', '0.75', '0.41', '0.16']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n",
      "Epi 58/200 | Avg Steps (last 100): 26.50 | Steps: 60 | Time: 50.22s\n",
      "Reward: 60.0\n",
      "  Losses -> Gen: 1.1705 (Rew: 1.1656, Dyn: 0.0031, Reg: 0.0018)\n",
      "  Policy -> Actor: -7.9648, Critic: 5.1108\n",
      "  Causal Probs (S->R): ['0.30', '0.75', '0.42', '0.16']\n",
      "  Compact Mask: [1. 1. 1. 0.]\n",
      "Epi 59/200 | Avg Steps (last 100): 27.95 | Steps: 112 | Time: 54.53s\n",
      "Reward: 112.0\n",
      "  Losses -> Gen: 13.7289 (Rew: 13.7160, Dyn: 0.0111, Reg: 0.0018)\n",
      "  Policy -> Actor: -8.4553, Critic: 1.0735\n",
      "  Causal Probs (S->R): ['0.30', '0.75', '0.42', '0.16']\n",
      "  Compact Mask: [1. 1. 1. 1.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 94\u001b[0m\n\u001b[0;32m     91\u001b[0m         generative_model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# Update Policy Model\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     actor_loss, critic_loss \u001b[38;5;241m=\u001b[39m \u001b[43msac_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerative_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\duk\\RL\\Generative-Reward-Decomposition-RL\\modules\\Agent.py:130\u001b[0m, in \u001b[0;36mDiscreteSACAgent.update\u001b[1;34m(self, transitions, generative_model)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    129\u001b[0m critic_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# --- Actor Update ---\u001b[39;00m\n\u001b[0;32m    133\u001b[0m action_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mget_action_dist(compact_state)\n",
      "File \u001b[1;32md:\\code\\python\\Lib\\site-packages\\torch\\optim\\optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m             )\n\u001b[1;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32md:\\code\\python\\Lib\\site-packages\\torch\\optim\\optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32md:\\code\\python\\Lib\\site-packages\\torch\\optim\\adam.py:226\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    217\u001b[0m         group,\n\u001b[0;32m    218\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    223\u001b[0m         state_steps,\n\u001b[0;32m    224\u001b[0m     )\n\u001b[1;32m--> 226\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\code\\python\\Lib\\site-packages\\torch\\optim\\optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\code\\python\\Lib\\site-packages\\torch\\optim\\adam.py:766\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    764\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 766\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\code\\python\\Lib\\site-packages\\torch\\optim\\adam.py:370\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    368\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    371\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(grad)\n\u001b[0;32m    372\u001b[0m     exp_avg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(exp_avg)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from modules import RewardModel, GenerativeModel, ReplayBuffer, DiscreteSACAgent, EpisodicRewardWrapper\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LR_POLICY = 3e-4          # Learning rate for actor and critic\n",
    "LR_GENERATIVE = 3e-4      # Learning rate for generative model\n",
    "GAMMA = 0.99              # Discount factor\n",
    "REPLAY_BUFFER_SIZE = 50000 # Size of the replay buffer\n",
    "BATCH_SIZE = 256          # Batch size for training\n",
    "TAU = 0.005               # Soft update coefficient for target networks\n",
    "ALPHA = 0.2               # SAC temperature parameter (entropy regularization)\n",
    "HIDDEN_DIM = 256          # Hidden dimension for neural networks\n",
    "MAX_EPISODES = 200       # Total number of episodes to run\n",
    "MAX_STEPS_PER_EPISODE = 500 # Max steps per episode for CartPole-v1\n",
    "START_TRAINING_EPISODES = 10 # Number of episodes to collect data before training starts\n",
    "\n",
    "# Hyperparameters for the GRD generative model loss (L_reg)\n",
    "# These control the sparsity of the learned causal graph. Increased to encourage sparsity.\n",
    "LAMBDA_S_R = 5e-4  # state -> reward\n",
    "LAMBDA_A_R = 1e-5  # action -> reward\n",
    "LAMBDA_S_S = 5e-5  # state -> state\n",
    "LAMBDA_A_S = 1e-8  # action -> state\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = EpisodicRewardWrapper(env)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "compact_state_dim = state_dim \n",
    "\n",
    "replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "generative_model = GenerativeModel(state_dim, action_dim, HIDDEN_DIM, DEVICE, LR_GENERATIVE, GAMMA, LAMBDA_S_S, LAMBDA_S_R, LAMBDA_A_S, LAMBDA_A_R)\n",
    "sac_agent = DiscreteSACAgent(state_dim, action_dim, compact_state_dim, HIDDEN_DIM, DEVICE, LR_POLICY, TAU, GAMMA, ALPHA)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"State dim: {state_dim}, Action dim: {action_dim}\")\n",
    "\n",
    "start_time = time.time()\n",
    "all_episode_steps = []\n",
    "\n",
    "rewards = []\n",
    "\n",
    "for i_episode in range(1, MAX_EPISODES + 1):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "    \n",
    "    episode_reward = 0\n",
    "    episode_states = []\n",
    "    episode_actions = []\n",
    "    \n",
    "    for t in range(MAX_STEPS_PER_EPISODE):\n",
    "        with torch.no_grad():\n",
    "            C_s_s, _, C_s_r, _ = generative_model.causal_module.get_causal_masks(training=False)\n",
    "            compact_mask = generative_model.causal_module.get_compact_representation_mask(C_s_s, C_s_r)\n",
    "            compact_state = state * compact_mask\n",
    "\n",
    "        action = sac_agent.select_action(compact_state)\n",
    "        \n",
    "        next_state_np, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # The episodic reward is only non-zero at the end\n",
    "        # The immediate reward for the replay buffer is this episodic reward if done, else 0\n",
    "        replay_reward = reward if done else 0.0\n",
    "        episode_reward += reward \n",
    "        \n",
    "        next_state = torch.tensor(next_state_np, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        \n",
    "        # Push the transition with the immediate (mostly zero) reward\n",
    "        replay_buffer.push(state, action, next_state, replay_reward, done)\n",
    "        episode_states.append(state)\n",
    "        episode_actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        if len(replay_buffer) > BATCH_SIZE and i_episode > START_TRAINING_EPISODES:\n",
    "            # Update Generative Model\n",
    "            generative_model.optimizer.zero_grad()\n",
    "            transitions = replay_buffer.sample(BATCH_SIZE)\n",
    "            trajectories = replay_buffer.sample_trajectories(4)\n",
    "            gen_loss, l_rew, l_dyn, l_reg = generative_model.calculate_loss(trajectories, transitions)\n",
    "            if torch.is_tensor(gen_loss):\n",
    "                gen_loss.backward()\n",
    "                generative_model.optimizer.step()\n",
    "\n",
    "            # Update Policy Model\n",
    "            actor_loss, critic_loss = sac_agent.update(transitions, generative_model)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    all_episode_steps.append(t + 1)\n",
    "\n",
    "    # After episode ends, store the full trajectory with the final episodic return\n",
    "    if episode_reward > 0:\n",
    "        ep_states_tensor = torch.cat(episode_states, dim=0)\n",
    "        ep_actions_tensor = torch.tensor(episode_actions, device=DEVICE, dtype=torch.long)\n",
    "        ep_return_tensor = torch.tensor([episode_reward], dtype=torch.float32, device=DEVICE)\n",
    "        replay_buffer.push_trajectory(ep_states_tensor, ep_actions_tensor, ep_return_tensor)\n",
    "\n",
    "    if i_episode % 1 == 0:\n",
    "        avg_steps = np.mean(all_episode_steps[-100:]) # Average over last 100 episodes\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Epi {i_episode}/{MAX_EPISODES} | Avg Steps (last 100): {avg_steps:.2f} | Steps: {t+1} | Time: {elapsed_time:.2f}s\")\n",
    "        print(f\"Reward: {episode_reward}\")\n",
    "        rewards.append(episode_reward)\n",
    "        if len(replay_buffer) > BATCH_SIZE and i_episode > START_TRAINING_EPISODES and torch.is_tensor(gen_loss):\n",
    "            print(f\"  Losses -> Gen: {gen_loss.item():.4f} (Rew: {l_rew.item():.4f}, Dyn: {l_dyn.item():.4f}, Reg: {l_reg.item():.4f})\")\n",
    "            print(f\"  Policy -> Actor: {actor_loss:.4f}, Critic: {critic_loss:.4f}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            C_s_s, _, C_s_r, C_a_r = generative_model.causal_module.get_causal_masks(training=False)\n",
    "            compact_mask = generative_model.causal_module.get_compact_representation_mask(C_s_s, C_s_r)\n",
    "            s_r_probs = F.softmax(generative_model.causal_module.s_to_r_logits, dim=-1)[:, 1].cpu().numpy()\n",
    "            a_r_probs = F.softmax(generative_model.causal_module.a_to_r_logits, dim=-1)[:, 1].cpu().numpy()\n",
    "            print(f\"  Causal Probs (S->R): {[f'{p:.2f}' for p in s_r_probs]}\")\n",
    "            print(f\"  Compact Mask: {compact_mask.cpu().numpy()}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60086a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 34.0,\n",
       " 14.0,\n",
       " 15.0,\n",
       " 45.0,\n",
       " 16.0,\n",
       " 10.0,\n",
       " 19.0,\n",
       " 11.0,\n",
       " 15.0,\n",
       " 50.0,\n",
       " 16.0,\n",
       " 30.0,\n",
       " 10.0,\n",
       " 22.0,\n",
       " 29.0,\n",
       " 10.0,\n",
       " 30.0,\n",
       " 17.0,\n",
       " 11.0,\n",
       " 22.0,\n",
       " 10.0,\n",
       " 32.0,\n",
       " 13.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 11.0,\n",
       " 26.0,\n",
       " 27.0,\n",
       " 17.0,\n",
       " 23.0,\n",
       " 43.0,\n",
       " 33.0,\n",
       " 23.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 24.0,\n",
       " 31.0,\n",
       " 36.0,\n",
       " 32.0,\n",
       " 84.0,\n",
       " 42.0,\n",
       " 29.0,\n",
       " 20.0,\n",
       " 23.0,\n",
       " 38.0,\n",
       " 31.0,\n",
       " 39.0,\n",
       " 25.0,\n",
       " 40.0,\n",
       " 49.0,\n",
       " 26.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 44.0,\n",
       " 60.0,\n",
       " 112.0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04dc3996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "nrewards = np.array(rewards, dtype=float)\n",
    "nrewards = np.cumsum(nrewards)\n",
    "plt.plot(range(len(nrewards)), nrewards)\n",
    "plt.show()\n",
    "\n",
    "with open('weights/rewards.pkl', 'wb') as f:\n",
    "    pkl.dump(rewards, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_agent.save('weights')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
